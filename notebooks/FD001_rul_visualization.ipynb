{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed25946b",
   "metadata": {},
   "source": [
    "# FD001 — RUL Predictions Visualization\n",
    "\n",
    "This notebook loads the saved LSTM sequence model and test data, computes per-window predictions, and visualizes predicted vs true Remaining Useful Life (RUL) for dataset **FD001**.\n",
    "\n",
    "**How to use:** make sure you have run the training and the following files exist in the repo root:\n",
    "\n",
    "- `data/processed/for_model/FD001_test_labeled.csv`\n",
    "- `models/sequence/FD001/lstm_best.pth`\n",
    "- `models/sequence/FD001/features.joblib`\n",
    "\n",
    "Then run the cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b1bb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "print('Imports ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c667bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      "TEST_CSV -> c:\\Users\\user\\BMW Projects\\auto-predictive-maintenance-system\\notebooks\\data\\processed\\for_model\\FD001_test_labeled.csv\n",
      "FEATURES_PKL -> c:\\Users\\user\\BMW Projects\\auto-predictive-maintenance-system\\notebooks\\models\\sequence\\FD001\\features.joblib\n",
      "CKPT_PATH -> c:\\Users\\user\\BMW Projects\\auto-predictive-maintenance-system\\notebooks\\models\\sequence\\FD001\\lstm_best.pth\n"
     ]
    }
   ],
   "source": [
    "# Configuration - adjust paths if your repo is elsewhere\n",
    "ROOT = Path.cwd()  # adjust if executing from a different working directory\n",
    "PROC_DIR = ROOT / 'data' / 'processed' / 'for_model'\n",
    "MODEL_DIR = ROOT / 'models' / 'sequence' / 'FD001'\n",
    "TEST_CSV = PROC_DIR / 'FD001_test_labeled.csv'\n",
    "FEATURES_PKL = MODEL_DIR / 'features.joblib'\n",
    "CKPT_PATH = MODEL_DIR / 'lstm_best.pth'\n",
    "\n",
    "print('Paths:')\n",
    "print('TEST_CSV ->', TEST_CSV)\n",
    "print('FEATURES_PKL ->', FEATURES_PKL)\n",
    "print('CKPT_PATH ->', CKPT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7097ec83",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load test dataframe and feature names\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mTEST_CSV\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest CSV not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTEST_CSV\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m FEATURES_PKL.exists(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFeatures not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFEATURES_PKL\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m CKPT_PATH.exists(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCheckpoint not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCKPT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "# Load test dataframe and feature names\n",
    "assert TEST_CSV.exists(), f\"Test CSV not found: {TEST_CSV}\"\n",
    "assert FEATURES_PKL.exists(), f\"Features not found: {FEATURES_PKL}\"\n",
    "assert CKPT_PATH.exists(), f\"Checkpoint not found: {CKPT_PATH}\"\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV)\n",
    "feature_names = joblib.load(FEATURES_PKL)\n",
    "print('Test rows:', len(df_test))\n",
    "print('Feature count:', len(feature_names))\n",
    "df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7404da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build sliding windows per unit and normalization similar to training\n",
    "SEQ_LEN = 50  # must match training seq_len\n",
    "\n",
    "# Build units grouped arrays\n",
    "units = []\n",
    "for unit, g in df_test.groupby('unit'):\n",
    "    g = g.sort_values('cycle').reset_index(drop=True)\n",
    "    feats = g[feature_names].to_numpy(dtype=np.float32)\n",
    "    targ = g['RUL_clipped'].to_numpy(dtype=np.float32) if 'RUL_clipped' in g.columns else g['RUL'].to_numpy(dtype=np.float32)\n",
    "    units.append({'unit': int(unit), 'cycles': g['cycle'].to_numpy(), 'features': feats, 'target': targ})\n",
    "\n",
    "# compute global mean/std from test units (we assume training saved mean/std but it wasn't persisted; using test-set stats is ok for quick viz)\n",
    "all_feats = np.vstack([u['features'] for u in units])\n",
    "mean = all_feats.mean(axis=0)\n",
    "std = all_feats.std(axis=0) + 1e-8\n",
    "\n",
    "print('Units found:', len(units))\n",
    "print('Feature dim:', units[0]['features'].shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTMRegressor (must match architecture used in training)\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, dropout=0.2, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * self.num_directions, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim//2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last).squeeze(1)\n",
    "\n",
    "print('Model class defined')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4fbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint, infer input dim from features\n",
    "input_dim = len(feature_names)\n",
    "print('Input dim:', input_dim)\n",
    "\n",
    "ckpt = torch.load(CKPT_PATH, map_location='cpu')\n",
    "# Attempt to detect hidden_dim & num_layers from state dict shapes\n",
    "state = ckpt.get('model_state', ckpt)\n",
    "# default hyperparams (fallbacks)\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "bidirectional = False\n",
    "\n",
    "# instantiate model\n",
    "model = LSTMRegressor(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, dropout=0.2, bidirectional=bidirectional)\n",
    "try:\n",
    "    model.load_state_dict(state)\n",
    "    print('Loaded state dict directly')\n",
    "except Exception as e:\n",
    "    # handle wrapper with 'model_state'\n",
    "    if 'model_state' in ckpt:\n",
    "        model.load_state_dict(ckpt['model_state'])\n",
    "        print('Loaded model_state from checkpoint')\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "model.eval()\n",
    "print('Model ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ec524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for each possible window per unit and align with cycles\n",
    "all_unit_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for u in units:\n",
    "        feats = u['features']\n",
    "        L, F = feats.shape\n",
    "        preds_per_cycle = np.full(L, np.nan, dtype=np.float32)\n",
    "        # sliding windows\n",
    "        for end in range(L):\n",
    "            start = max(0, end - SEQ_LEN + 1)\n",
    "            seq = feats[start:end+1]\n",
    "            # pad if shorter\n",
    "            if seq.shape[0] < SEQ_LEN:\n",
    "                pad_len = SEQ_LEN - seq.shape[0]\n",
    "                pad = np.zeros((pad_len, seq.shape[1]), dtype=np.float32)\n",
    "                seq = np.vstack([pad, seq])\n",
    "            # normalize\n",
    "            seq = (seq - mean) / std\n",
    "            seq_tensor = torch.from_numpy(seq).unsqueeze(0)  # (1, T, F)\n",
    "            pred = model(seq_tensor).cpu().numpy().item()\n",
    "            preds_per_cycle[end] = pred\n",
    "        all_unit_preds.append({'unit': u['unit'], 'cycles': u['cycles'], 'preds': preds_per_cycle, 'targets': u['target']})\n",
    "\n",
    "print('Predictions generated for all units')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each unit, take the prediction at the last observed cycle and compare to true RUL at last cycle\n",
    "preds = []\n",
    "trues = []\n",
    "units_list = []\n",
    "for u in all_unit_preds:\n",
    "    last_pred = float(u['preds'][-1])\n",
    "    last_true = float(u['targets'][-1])\n",
    "    preds.append(last_pred)\n",
    "    trues.append(last_true)\n",
    "    units_list.append(u['unit'])\n",
    "\n",
    "preds = np.array(preds)\n",
    "trues = np.array(trues)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(trues, preds)\n",
    "rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "r2 = r2_score(trues, preds)\n",
    "print('Per-unit last-cycle metrics:')\n",
    "print('MAE =', mae)\n",
    "print('RMSE =', rmse)\n",
    "print('R2 =', r2)\n",
    "\n",
    "# Build DataFrame for plotting\n",
    "df_eval = pd.DataFrame({'unit': units_list, 'true_rul': trues, 'pred_rul': preds})\n",
    "df_eval = df_eval.sort_values('unit').reset_index(drop=True)\n",
    "df_eval.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter: True vs Predicted RUL (last observed cycle)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(df_eval['true_rul'], df_eval['pred_rul'], alpha=0.7)\n",
    "plt.plot([df_eval['true_rul'].min(), df_eval['true_rul'].max()], [df_eval['true_rul'].min(), df_eval['true_rul'].max()], 'r--')\n",
    "plt.xlabel('True RUL')\n",
    "plt.ylabel('Predicted RUL')\n",
    "plt.title('FD001: True vs Predicted (last observed cycle)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7952eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error histogram\n",
    "errors = df_eval['pred_rul'] - df_eval['true_rul']\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(errors, bins=30)\n",
    "plt.title('Prediction Error Histogram (pred - true)')\n",
    "plt.xlabel('Error (cycles)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Show top worst units\n",
    "df_eval['abs_err'] = errors.abs()\n",
    "print('Top 10 worst units by absolute error:')\n",
    "print(df_eval.sort_values('abs_err', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de479b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time-series predictions vs true RUL for a few example units (worst, median, best)\n",
    "worst_units = df_eval.sort_values('abs_err', ascending=False).head(3)['unit'].tolist()\n",
    "median_unit = df_eval.sort_values('abs_err').iloc[len(df_eval)//2]['unit']\n",
    "example_units = worst_units + [median_unit]\n",
    "\n",
    "for u_id in example_units:\n",
    "    u = next(x for x in all_unit_preds if x['unit'] == u_id)\n",
    "    cycles = u['cycles']\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.plot(cycles, u['targets'], label='true RUL')\n",
    "    plt.plot(cycles, u['preds'], label='predicted RUL')\n",
    "    plt.xlabel('Cycle')\n",
    "    plt.ylabel('RUL')\n",
    "    plt.title(f'Unit {u_id} — True vs Predicted RUL (all cycles)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df58a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save per-unit evaluation to CSV for later reporting\n",
    "out_path = ROOT / 'analysis' / 'fd001_unit_predictions.csv'\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_eval.to_csv(out_path, index=False)\n",
    "print('Saved per-unit eval to', out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
